{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_TYPE = 'simple_lstm_only_flows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from place_groups import selected_junctions, selected_middle_of_roads\n",
    "from flow_data_maker import default_bootstrap_flow_data_maker\n",
    "from common import *\n",
    "\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import timedelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pandas as pd\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lstm_state_size, num_lags, num_outs, loss):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_state_size, input_shape=(num_lags, 1), return_sequences=False))\n",
    "    model.add(Dense(units=num_outs, activation=\"linear\"))  # Linear activation, because speed RESIDUALS can have any sign.\n",
    "    model.compile(loss=loss, optimizer=\"rmsprop\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_training(history):\n",
    "    plt.plot(history.history['loss'], linestyle='--')\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss in Train Phase')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(loss, lstm_state_size, num_lags, num_outs,\n",
    "                           place_id, data_maker, mini_batch_size, num_epochs, validation_split):\n",
    "    checkpoint_path = './hdf5/%d.hdf5' % random.randint(2**40, 2**41)\n",
    "    print('Creating model %s' % checkpoint_path)\n",
    "    x_train, y_train, _, _ = data_maker.get_train_and_test_inputs()\n",
    "    model = build_model(lstm_state_size=lstm_state_size, num_lags=num_lags, num_outs=num_outs, loss=loss)\n",
    "    history = model.fit(\n",
    "        np.expand_dims(np.array(x_train.values), 2),\n",
    "        y_train.values,\n",
    "        batch_size=mini_batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=[ModelCheckpoint(\n",
    "            checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')],\n",
    "        verbose=2)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    plot_model_training(history)\n",
    "    return model, history, data_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model ./hdf5/1415746309628.hdf5\n",
      "Train on 34780 samples, validate on 8695 samples\n",
      "Epoch 1/100\n",
      "Epoch 00001: val_loss improved from inf to 17.07306, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 4s - loss: 31.3648 - val_loss: 17.0731\n",
      "Epoch 2/100\n",
      "Epoch 00002: val_loss improved from 17.07306 to 3.31355, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 8.8522 - val_loss: 3.3135\n",
      "Epoch 3/100\n",
      "Epoch 00003: val_loss improved from 3.31355 to 1.15412, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 1.9657 - val_loss: 1.1541\n",
      "Epoch 4/100\n",
      "Epoch 00004: val_loss improved from 1.15412 to 0.74032, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.8812 - val_loss: 0.7403\n",
      "Epoch 5/100\n",
      "Epoch 00005: val_loss improved from 0.74032 to 0.51572, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.6575 - val_loss: 0.5157\n",
      "Epoch 6/100\n",
      "Epoch 00006: val_loss improved from 0.51572 to 0.47583, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.5761 - val_loss: 0.4758\n",
      "Epoch 7/100\n",
      "Epoch 00007: val_loss improved from 0.47583 to 0.42075, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.5315 - val_loss: 0.4207\n",
      "Epoch 8/100\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 2s - loss: 0.4987 - val_loss: 0.4617\n",
      "Epoch 9/100\n",
      "Epoch 00009: val_loss improved from 0.42075 to 0.41003, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.4730 - val_loss: 0.4100\n",
      "Epoch 10/100\n",
      "Epoch 00010: val_loss improved from 0.41003 to 0.40356, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.4549 - val_loss: 0.4036\n",
      "Epoch 11/100\n",
      "Epoch 00011: val_loss improved from 0.40356 to 0.34828, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.4268 - val_loss: 0.3483\n",
      "Epoch 12/100\n",
      "Epoch 00012: val_loss improved from 0.34828 to 0.34667, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.4124 - val_loss: 0.3467\n",
      "Epoch 13/100\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 2s - loss: 0.3870 - val_loss: 0.4097\n",
      "Epoch 14/100\n",
      "Epoch 00014: val_loss improved from 0.34667 to 0.34518, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.3759 - val_loss: 0.3452\n",
      "Epoch 15/100\n",
      "Epoch 00015: val_loss improved from 0.34518 to 0.31047, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.3668 - val_loss: 0.3105\n",
      "Epoch 16/100\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 2s - loss: 0.3536 - val_loss: 0.3375\n",
      "Epoch 17/100\n",
      "Epoch 00017: val_loss improved from 0.31047 to 0.29799, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.3476 - val_loss: 0.2980\n",
      "Epoch 18/100\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 2s - loss: 0.3494 - val_loss: 0.3502\n",
      "Epoch 19/100\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 2s - loss: 0.3409 - val_loss: 0.3400\n",
      "Epoch 20/100\n",
      "Epoch 00020: val_loss improved from 0.29799 to 0.28948, saving model to ./hdf5/1415746309628.hdf5\n",
      " - 2s - loss: 0.3349 - val_loss: 0.2895\n",
      "Epoch 21/100\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 2s - loss: 0.3351 - val_loss: 0.2987\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-30eba225c37f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c91e64e3bdc3>\u001b[0m in \u001b[0;36mcreate_and_train_model\u001b[0;34m(loss, lstm_state_size, num_lags, num_outs, place_id, data_maker, mini_batch_size, num_epochs, validation_split)\u001b[0m\n\u001b[1;32m     13\u001b[0m         callbacks=[ModelCheckpoint(\n\u001b[1;32m     14\u001b[0m             checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')],\n\u001b[0;32m---> 15\u001b[0;31m         verbose=2)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mplot_model_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/deep_learning_project_norrecampus/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# model, history, data_maker = create_and_train_model(\n",
    "#     loss='mse', \n",
    "#     lstm_state_size=20, \n",
    "#     num_lags=NUM_LAGS, \n",
    "#     num_outs=1,\n",
    "#     place_id=EXAMPLE_PLACE_ID, \n",
    "#     data_maker=default_bootstrap_flow_data_maker(EXAMPLE_PLACE_ID),\n",
    "#     mini_batch_size=512, \n",
    "#     num_epochs=100, \n",
    "#     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, speed_data_maker):\n",
    "    _, _, x_test_normalized, y_test_normalized = speed_data_maker.get_train_and_test_inputs()\n",
    "    predictions_normalized = model.predict(np.expand_dims(x_test_normalized, 2)).flatten()\n",
    "    errors_df = speed_data_maker.individual_errors_without_interpolated_values(predictions_normalized)\n",
    "    return compute_error_statistics(errors_df, 'speed_km_hr_true', 'speed_km_hr_predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def one_place(place_id):\n",
    "    model, history, speed_data_maker = create_and_train_model(\n",
    "        loss='mse', \n",
    "        lstm_state_size=20, \n",
    "        num_lags=NUM_LAGS, \n",
    "        num_outs=1,\n",
    "        place_id=place_id, \n",
    "        speed_data_maker=default_bootstrap_speed_data_maker(place_id),\n",
    "        mini_batch_size=512, \n",
    "        num_epochs=100, \n",
    "        validation_split=0.2)\n",
    "    return pd.DataFrame({NN_TYPE: predict(model, speed_data_maker)})\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={'index': 'stat'})\\\n",
    "        .assign(place_id=place_id)\\\n",
    "        .set_index(['place_id', 'stat'])\n",
    "\n",
    "def all_places(group, name):\n",
    "    pd.concat(map(one_place, group)).to_csv(name + '.csv')\n",
    "    results = pd.read_csv('%s.csv' % name).groupby('stat').mean()\n",
    "    results.to_csv('%s_summary.csv' % name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_middles = all_places(selected_middle_of_roads, NN_TYPE + '_middles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_middles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_junctions = all_places(selected_junctions, NN_TYPE + '_junctions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_junctions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
