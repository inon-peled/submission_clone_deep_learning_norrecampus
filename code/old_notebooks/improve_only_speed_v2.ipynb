{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from place_groups import place_groups_middle_of_roads, place_groups_junctions\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import timedelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pandas as pd\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5_PREFIX = 'only_speed_lstm_with_dropout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5_DIR = os.path.join('.', 'hdf5')\n",
    "ONE_DAY_LAGS = 288\n",
    "DATA_PATH = '/mnt/sdc1/inon/norrecampus/data/by_place_5min'\n",
    "EXAMPLE_PLACE_ID = 'ChIJZaR1M1hSUkYRxP0WkwYYy_k'\n",
    "\n",
    "THRESHOLD_HIGH_OUTLIER = 110\n",
    "LOSS = 'mse'\n",
    "MINI_BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_LAGS = 12\n",
    "LSTM_STATE_SIZE = NUM_LAGS\n",
    "NUM_OUTS_MEAN_SPEED_REGRESSION = 1\n",
    "SPLIT_DATE = '2015-06-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_timestamp(ser):\n",
    "    return pd\\\n",
    "        .DataFrame(ser)\\\n",
    "        .assign(day_of_week=lambda df: df.index.dayofweek, time_of_day=lambda df: df.index.time)\\\n",
    "        .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnerForMeanSpeedOfOneSegment(object):\n",
    "    def __init__(self,\n",
    "                 dropout_factor,\n",
    "                 threshold_high_outlier,\n",
    "                 loss,\n",
    "                 place_id, \n",
    "                 mini_batch_size,\n",
    "                 num_epochs,\n",
    "                 validation_split,\n",
    "                 num_lags,\n",
    "                 lstm_state_size,\n",
    "                 num_outs,\n",
    "                 split_date):\n",
    "        self.df = None\n",
    "        self.model = None\n",
    "        \n",
    "        self.dropout_factor = dropout_factor\n",
    "        self.threshold_high_outlier = threshold_high_outlier\n",
    "        self.loss = loss\n",
    "        self.place_id = place_id\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.validation_split = validation_split\n",
    "        self.num_lags = num_lags\n",
    "        self.lstm_state_size = lstm_state_size\n",
    "        self.num_outs = num_outs\n",
    "        self.split_date = split_date\n",
    "        self.checkpoint_basename = os.path.join(HDF5_DIR, '%s_%.2f_%s.best.hdf5' % \n",
    "                                                (HDF5_PREFIX, self.dropout_factor, self.place_id))\n",
    "    \n",
    "    def compute_error_statistics(self, errors_df):\n",
    "        abs_errors = errors_df.error.abs()\n",
    "        abs_errors_normalized = abs_errors / errors_df.speed_km_hr_true\n",
    "        return pd.DataFrame({\n",
    "            'corr': np.corrcoef(errors_df.speed_km_hr_predicted, errors_df.speed_km_hr_true)[0, 1],\n",
    "            'mae': np.mean(abs_errors),\n",
    "            'mape': np.mean(abs_errors_normalized),\n",
    "            'mse': np.mean(abs_errors ** 2),\n",
    "            'msne': np.mean(abs_errors_normalized ** 2),\n",
    "            'rae': np.sum(abs_errors) / np.sum(np.abs(errors_df.speed_km_hr_true - np.mean(errors_df.speed_km_hr_true))),\n",
    "            'rmse': np.sqrt(np.mean(abs_errors ** 2)),\n",
    "            'rmsne': np.sqrt(np.mean(abs_errors_normalized ** 2)),\n",
    "            'r2': max(0, 1 - np.sum(abs_errors ** 2) / np.sum((errors_df.speed_km_hr_true - np.mean(errors_df.speed_km_hr_true)) ** 2))\n",
    "        }, index=[self.place_id])\n",
    "    \n",
    "    def baseline_lr(self):\n",
    "        X_train_normalized, Y_train_normalized, X_test_normalized, Y_test_normalized = self._get_train_and_test_inputs()\n",
    "        trained_lr = LinearRegression(fit_intercept=False).fit(X_train_normalized.values, Y_train_normalized.values)\n",
    "        lr_predictions_normalized = trained_lr.predict(X_test_normalized)\n",
    "        errors_df = self._individual_errors_without_interpolated_values(lr_predictions_normalized)\n",
    "        return self.compute_error_statistics(errors_df)\n",
    "        \n",
    "    def _get_detrend_factors(self):\n",
    "        ser_imp = self._imputation_for_missing_values_and_outliers(self._get_df()).speed_km_hr\n",
    "        df_train_period = ser_imp[lambda df: df.index < self.split_date]\n",
    "        return df_train_period\\\n",
    "            .groupby([df_train_period.index.dayofweek, df_train_period.index.time])\\\n",
    "            .agg(['mean', 'std'])\\\n",
    "            .reset_index()\\\n",
    "            .rename(columns={'level_0': 'day_of_week', 'level_1': 'time_of_day'})\n",
    "        \n",
    "    def detrend(self, ser):\n",
    "        return pd\\\n",
    "            .merge(left=split_timestamp(ser), \n",
    "                   right=self._get_detrend_factors(),\n",
    "                   on=['day_of_week', 'time_of_day'], \n",
    "                   how='inner')\\\n",
    "                .assign(speed_normalized=lambda df: (df.speed_km_hr - df['mean']) / df['std'])\\\n",
    "                .set_index('index')\\\n",
    "                .speed_normalized\\\n",
    "                .sort_index()\n",
    "\n",
    "    def _map_back(self, detrended_series):\n",
    "        return pd\\\n",
    "            .merge(left=split_timestamp(detrended_series), \n",
    "                   right=self._get_detrend_factors(),\n",
    "                   on=['day_of_week', 'time_of_day'], \n",
    "                   how='inner')\\\n",
    "            .assign(speed_km_hr=lambda df: (df.speed_normalized * df['std']) + df['mean'])\\\n",
    "            .set_index('index')\\\n",
    "            .speed_km_hr\\\n",
    "            .sort_index()\n",
    "    \n",
    "    def _imputation_for_missing_values_and_outliers(self, ser):\n",
    "        missing_timestamps = pd\\\n",
    "            .date_range(min(ser.index.date), max(ser.index.date) + timedelta(days=1), freq='5T')\\\n",
    "            .difference(ser.index)\n",
    "        df_with_nans_where_missing = pd.DataFrame(ser)\\\n",
    "            .join(pd.DataFrame(index=missing_timestamps), how='outer')\\\n",
    "            .assign(original_value=lambda df: df.iloc[:, 0])\n",
    "        return df_with_nans_where_missing\\\n",
    "            .iloc[:, 0]\\\n",
    "            .mask(lambda df: df > self.threshold_high_outlier)\\\n",
    "            .interpolate()\\\n",
    "            .to_frame()\\\n",
    "            .assign(original_value=df_with_nans_where_missing.original_value)\\\n",
    "            .assign(is_interpolated=lambda df: df.iloc[:, 0] != df.iloc[:, 1])\n",
    "        \n",
    "    def _get_df(self):\n",
    "        if self.df is None:\n",
    "            self.df = pd.read_csv(\n",
    "                os.path.join(DATA_PATH, self.place_id + '.csv'),\n",
    "                parse_dates=['start_interval_s', 'end_interval_s']\\\n",
    "            )[lambda df: df.start_interval_s >= '2015-01-01']\\\n",
    "            .rename(columns={'start_interval_s': 't', 'speed_mean': 'speed_km_hr'})\\\n",
    "            .set_index('t')\\\n",
    "            .speed_km_hr\n",
    "        return self.df\n",
    "\n",
    "    def _get_train_and_test_inputs(self):\n",
    "        speeds_interpolated_and_detrended = self.detrend(\n",
    "            self._imputation_for_missing_values_and_outliers(self._get_df()).speed_km_hr)\n",
    "        lags = pd.concat([speeds_interpolated_and_detrended.shift(x) for x in range(self.num_lags + 1)], axis=1)[self.num_lags:]\n",
    "        train = lags[lags.index < self.split_date]\n",
    "        X_train = train.iloc[:, 1:]\n",
    "        Y_train = train.iloc[:, 0]\n",
    "        test = lags[lags.index >= self.split_date]\n",
    "        X_test = test.iloc[:, 1:]\n",
    "        Y_test = test.iloc[:, 0]\n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "    def _build_model(self):\n",
    "        # TODO: better weights initialization\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(self.lstm_state_size, input_shape=(self.num_lags, 1), return_sequences=False))\n",
    "        self.model.add(Dropout(self.dropout_factor))\n",
    "        #model.add(Dense(units=1, kernel_regularizer=regularizers.l2(0.00001)))\n",
    "        self.model.add(Dense(units=self.num_outs, activation=\"linear\"))  # Linear activation, because speed RESIDUALS can have any sign.\n",
    "        self.model.compile(loss=self.loss, optimizer=\"rmsprop\")  # TODO: try adam optimizer too, although rmsprop is the default go-to for RNN\n",
    "    \n",
    "    def load_best_model_from_disk(self):\n",
    "        self._build_model()\n",
    "        self.model.load_weights(self.checkpoint_basename)\n",
    "    \n",
    "    def create_and_train_model(self):\n",
    "        self._build_model()\n",
    "        X_train, Y_train, _, _ = self._get_train_and_test_inputs()\n",
    "        self.model.fit(\n",
    "            np.expand_dims(np.array(X_train.values), 2),\n",
    "            Y_train.values,\n",
    "            batch_size=self.mini_batch_size,\n",
    "            epochs=self.num_epochs,\n",
    "            validation_split=self.validation_split,\n",
    "            # checkpoint best model\n",
    "            callbacks=[ModelCheckpoint(\n",
    "                self.checkpoint_basename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')],\n",
    "            verbose=2)\n",
    "        self.model.load_weights(self.checkpoint_basename)\n",
    "    \n",
    "    def predict(self):\n",
    "        _, _, X_test_normalized, Y_test_normalized = self._get_train_and_test_inputs()\n",
    "        predictions_normalized = self.model.predict(np.expand_dims(X_test_normalized, 2)).flatten()\n",
    "        errors_df = self._individual_errors_without_interpolated_values(predictions_normalized)\n",
    "        return self.compute_error_statistics(errors_df)\n",
    "        \n",
    "    def _impute(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _individual_errors_without_interpolated_values(self, predictions_normalized):\n",
    "        _, _, _, Y_test_normalized = self._get_train_and_test_inputs()\n",
    "        ser_Y_test = self._map_back(Y_test_normalized)\n",
    "        ser_predictions = self._map_back(pd.Series(\n",
    "            predictions_normalized, index=Y_test_normalized.index, name='speed_normalized'))\n",
    "        interpolated_timestamps = self\\\n",
    "            ._imputation_for_missing_values_and_outliers(self._get_df())\\\n",
    "            [lambda df: df.is_interpolated]\\\n",
    "            .index\n",
    "        return ser_Y_test.to_frame()\\\n",
    "            .join(ser_predictions.to_frame(), lsuffix='_true', rsuffix='_predicted')\\\n",
    "            .loc[lambda df: df.index.difference(interpolated_timestamps)]\\\n",
    "            .assign(error=lambda df: df.speed_km_hr_true - df.speed_km_hr_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_place_improvement_over_baseline_lr(place_id, dropout_factor):\n",
    "    learner = LearnerForMeanSpeedOfOneSegment(\n",
    "        dropout_factor=dropout_factor,\n",
    "        threshold_high_outlier=THRESHOLD_HIGH_OUTLIER,\n",
    "        loss=LOSS,\n",
    "        place_id=place_id, \n",
    "        mini_batch_size=MINI_BATCH_SIZE,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        num_lags=NUM_LAGS,\n",
    "        lstm_state_size=LSTM_STATE_SIZE,\n",
    "        num_outs=NUM_OUTS_MEAN_SPEED_REGRESSION,\n",
    "        split_date=SPLIT_DATE)\n",
    "    learner.create_and_train_model()\n",
    "    learner.load_best_model_from_disk()\n",
    "    return learner.predict() - learner.baseline_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_place_improvement_over_baseline_lr(EXAMPLE_PLACE_ID, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_places(place_group, place_group_name):\n",
    "    places = [lst[len(lst) // 2] for lst in place_group]\n",
    "    for dropout_factor in [0.25, 0.5, 0.75]:\n",
    "        return pd.concat(map(lambda place_id: one_place_improvement_over_baseline_lr(place_id, dropout_factor), places))\\\n",
    "            .to_csv('%s_dropout_%.2f_improvement_over_baseline_lr_v2.csv' % (dropout_factor, place_group_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34780 samples, validate on 8696 samples\n",
      "Epoch 1/100\n",
      "Epoch 00001: val_loss improved from inf to 0.77488, saving model to ./hdf5/only_speed_lstm_with_dropout_0.25_ChIJ7feGfFdSUkYRmmsyva5Yh7g.best.hdf5\n",
      " - 2s - loss: 0.9475 - val_loss: 0.7749\n",
      "Epoch 2/100\n",
      "Epoch 00002: val_loss improved from 0.77488 to 0.66157, saving model to ./hdf5/only_speed_lstm_with_dropout_0.25_ChIJ7feGfFdSUkYRmmsyva5Yh7g.best.hdf5\n",
      " - 1s - loss: 0.8445 - val_loss: 0.6616\n",
      "Epoch 3/100\n",
      "Epoch 00003: val_loss improved from 0.66157 to 0.55578, saving model to ./hdf5/only_speed_lstm_with_dropout_0.25_ChIJ7feGfFdSUkYRmmsyva5Yh7g.best.hdf5\n",
      " - 1s - loss: 0.7181 - val_loss: 0.5558\n",
      "Epoch 4/100\n",
      "Epoch 00004: val_loss improved from 0.55578 to 0.47879, saving model to ./hdf5/only_speed_lstm_with_dropout_0.25_ChIJ7feGfFdSUkYRmmsyva5Yh7g.best.hdf5\n",
      " - 1s - loss: 0.6205 - val_loss: 0.4788\n",
      "Epoch 5/100\n",
      "Epoch 00005: val_loss improved from 0.47879 to 0.44697, saving model to ./hdf5/only_speed_lstm_with_dropout_0.25_ChIJ7feGfFdSUkYRmmsyva5Yh7g.best.hdf5\n",
      " - 1s - loss: 0.5584 - val_loss: 0.4470\n",
      "Epoch 6/100\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.5270 - val_loss: 0.4719\n",
      "Epoch 7/100\n"
     ]
    }
   ],
   "source": [
    "multiple_places(place_groups_middle_of_roads, 'place_groups_middle_of_roads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_places(place_groups_junctions, 'place_groups_junctions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
